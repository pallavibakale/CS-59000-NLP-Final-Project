{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Summarization using SentenceBert & LegalBert**"
      ],
      "metadata": {
        "id": "qmr_rs1XaJAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "!pip install transformers torch\n",
        "!pip install ftfy\n",
        "!pip install spacy\n",
        "!pip install datasets\n",
        "!pip install rouge-score\n",
        "# Load spaCy's English model\n",
        "!python -m spacy download en_core_web_sm\n",
        "# Import the spacy module\n",
        "import spacy # This line is added to import spacy into the current scope\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "import ftfy\n",
        "import shutil\n",
        "import chardet\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, EncoderDecoderModel\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "W9nJ6IpYWDSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jN8RbX7l7SFO"
      },
      "outputs": [],
      "source": [
        "# Define the input zip files and output directories\n",
        "zip_file_1 = \"ToSDRData.zip\"\n",
        "zip_file_2 = \"ReferenceSummaries.zip\"\n",
        "output_dir_1 = \"DataSet\"\n",
        "output_dir_2 = \"ReferenceSummaries\"\n",
        "\n",
        "def recreate_folder(folder_path):\n",
        "    # If the folder exists, delete it\n",
        "    if os.path.exists(folder_path):\n",
        "        shutil.rmtree(folder_path)\n",
        "    # Recreate the folder\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "# Recreate the output folders\n",
        "recreate_folder(output_dir_1)\n",
        "recreate_folder(output_dir_2)\n",
        "\n",
        "# Function to extract files from a zip\n",
        "def extract_files(zip_file, output_dir, folder_name):\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        # List all files in the zip\n",
        "        all_files = zip_ref.namelist()\n",
        "\n",
        "        # Filter files in the specific folder (e.g., 'text/')\n",
        "        folder_files = [f for f in all_files if f.startswith(folder_name + '/') and not f.endswith('/')]\n",
        "\n",
        "        # Extract only the files in the specified folder\n",
        "        for file in folder_files:\n",
        "            # Determine the target path in the output folder\n",
        "            target_path = os.path.join(output_dir, os.path.relpath(file, folder_name))\n",
        "\n",
        "            # Ensure the directory structure exists\n",
        "            os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
        "\n",
        "            # Extract the file\n",
        "            with zip_ref.open(file) as source, open(target_path, 'wb') as target:\n",
        "                target.write(source.read())\n",
        "\n",
        "# Extract files from 'text' folder in ToSDRData.zip into DataSet\n",
        "extract_files(zip_file_1, output_dir_1, 'text')\n",
        "\n",
        "# Extract files from 'text' folder in ReferenceSummaries.zip into ReferenceSummaries\n",
        "extract_files(zip_file_2, output_dir_2, 'text')\n",
        "\n",
        "print(f\"All files from 'text' folder in {zip_file_1} have been extracted to {output_dir_1}.\")\n",
        "print(f\"All files from 'text' folder in {zip_file_2} have been extracted to {output_dir_2}.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-processing\n",
        "# Load spaCy model for sentence segmentation\n",
        "nlp = spacy.blank(\"en\")\n",
        "nlp.add_pipe(\"sentencizer\")  # Add the sentencizer component\n",
        "\n",
        "# Input and output folder paths\n",
        "input_folder = \"DataSet\"\n",
        "output_folder = \"PreprocessedDataSet\"\n",
        "\n",
        "def recreate_folder(folder_path):\n",
        "    # If the folder exists, delete it\n",
        "    if os.path.exists(folder_path):\n",
        "        shutil.rmtree(folder_path)\n",
        "    # Recreate the folder\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "# Recreate the output folder\n",
        "recreate_folder(output_folder)\n",
        "\n",
        "# Function to remove HTML tags but retain URLs\n",
        "def remove_html_tags(text):\n",
        "    # Parse HTML content using BeautifulSoup\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "\n",
        "    # Find and preserve URLs\n",
        "    urls = [a['href'] for a in soup.find_all('a', href=True)]\n",
        "\n",
        "    # Extract the plain text\n",
        "    plain_text = soup.get_text(separator=\" \")\n",
        "\n",
        "    # Reinsert URLs into the text\n",
        "    for url in urls:\n",
        "        plain_text += f\" {url}\"\n",
        "\n",
        "    return plain_text\n",
        "\n",
        "# Function to preprocess a single file\n",
        "def preprocess_text(text):\n",
        "    # Step 1: Remove HTML tags but retain URLs\n",
        "    text = remove_html_tags(text)\n",
        "\n",
        "    # Step 2: Remove special characters and normalize white spaces\n",
        "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)  # Remove non-ASCII characters\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize spaces and remove leading/trailing spaces\n",
        "\n",
        "    # Step 3: Retain enumerations (e.g., 1.1, A, i., etc.)\n",
        "    enumerations = re.findall(r\"(^|\\s)(\\d+\\.\\d+|[A-Za-z]|[ivxIVX]+)(?=[\\.\\)]\\s)\", text)\n",
        "    enumerations = {e[1] for e in enumerations}\n",
        "\n",
        "    # Step 4: Break into sentences for better readability\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents]\n",
        "\n",
        "    # Step 5: Highlight enumerations as important points\n",
        "    important_points = [sent for sent in sentences if any(e in sent for e in enumerations)]\n",
        "\n",
        "    # Combine all text and important points\n",
        "    processed_text = \"\\n\".join(important_points + sentences)\n",
        "    return processed_text\n",
        "\n",
        "# Loop through all files in the input folder\n",
        "for file_name in os.listdir(input_folder):\n",
        "    input_path = os.path.join(input_folder, file_name)\n",
        "\n",
        "    # Ensure we process only text files\n",
        "    if os.path.isfile(input_path) and input_path.endswith(\".txt\"):\n",
        "        # Read the file content\n",
        "        with open(input_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            content = file.read()\n",
        "\n",
        "        # Preprocess the text\n",
        "        preprocessed_content = preprocess_text(content)\n",
        "\n",
        "        # Save the preprocessed text to the output folder\n",
        "        output_path = os.path.join(output_folder, file_name)\n",
        "        with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(preprocessed_content)\n",
        "\n",
        "print(f\"Preprocessing completed. Files saved in '{output_folder}'.\")\n"
      ],
      "metadata": {
        "id": "AZh0HjCzDeRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, EncoderDecoderModel, BertForSequenceClassification\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AdamW\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Paths\n",
        "preprocessed_folder = \"PreprocessedDataSet\"\n",
        "reference_folder = \"ReferenceSummaries\"\n",
        "annotations_file = \"Annotations.csv\"  # CSV file containing 'text' and 'summary'\n",
        "\n",
        "# Hyperparameters\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 10\n",
        "LEARNING_RATE = 1e-5\n",
        "MAX_INPUT_LENGTH = 512\n",
        "MAX_TARGET_LENGTH = 128\n",
        "CHUNK_SIZE = 512\n",
        "CHUNK_OVERLAP = 50\n",
        "\n",
        "# Initialize Tokenizer and Model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
        "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
        "    \"nlpaueb/legal-bert-base-uncased\", \"nlpaueb/legal-bert-base-uncased\"\n",
        ")\n",
        "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.eos_token_id = tokenizer.sep_token_id\n",
        "\n",
        "# Initialize Sentence-BERT model\n",
        "sentence_bert = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Function to chunk text\n",
        "def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), chunk_size - overlap):\n",
        "        chunk = tokens[i:i + chunk_size]\n",
        "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
        "    return chunks\n",
        "\n",
        "# Dataset Class\n",
        "class LegalDataset(Dataset):\n",
        "    def __init__(self, input_folder, reference_folder, file_list, tokenizer, sentence_bert, annotations_df, max_input_len, max_target_len):\n",
        "        self.input_folder = input_folder\n",
        "        self.reference_folder = reference_folder\n",
        "        self.file_list = file_list\n",
        "        self.tokenizer = tokenizer\n",
        "        self.sentence_bert = sentence_bert\n",
        "        self.annotations_df = annotations_df\n",
        "        self.max_input_len = max_input_len\n",
        "        self.max_target_len = max_target_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_name = self.file_list[idx]\n",
        "        # Load input text\n",
        "        with open(os.path.join(self.input_folder, file_name), \"r\", encoding=\"utf-8\") as f:\n",
        "            input_text = f.read().strip()\n",
        "        # Chunk the input text\n",
        "        input_chunks = chunk_text(input_text)\n",
        "\n",
        "        # Load reference summary\n",
        "        with open(os.path.join(self.reference_folder, file_name), \"r\", encoding=\"utf-8\") as f:\n",
        "            target_text = f.read().strip()\n",
        "\n",
        "        # Tokenize the first chunk (for simplicity, you can modify this to include multiple chunks)\n",
        "        input_tokens = self.tokenizer(\n",
        "            input_chunks[0], max_length=self.max_input_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "        target_tokens = self.tokenizer(\n",
        "            target_text, max_length=self.max_target_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Match the sentence in the chunk to its corresponding summary using Sentence-BERT\n",
        "        text_embedding = self.sentence_bert.encode(input_chunks[0])\n",
        "        summary_embedding = self.sentence_bert.encode(target_text)\n",
        "\n",
        "        # Calculate cosine similarity (optional, can be used for selecting relevant sentences)\n",
        "        cosine_sim = cosine_similarity([text_embedding], [summary_embedding])[0][0]\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_tokens[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": input_tokens[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": target_tokens[\"input_ids\"].squeeze(),\n",
        "            \"cosine_sim\": cosine_sim,\n",
        "        }\n",
        "\n",
        "# Prepare Training Data\n",
        "file_list = sorted(os.listdir(preprocessed_folder))[:15]\n",
        "annotations_df = pd.read_csv(annotations_file)\n",
        "dataset = LegalDataset(preprocessed_folder, reference_folder, file_list, tokenizer, sentence_bert, annotations_df, MAX_INPUT_LENGTH, MAX_TARGET_LENGTH)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Track learning rate and loss for graph\n",
        "learning_rates = []\n",
        "losses = []\n",
        "\n",
        "# Training Loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(\"Starting Training...\")\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    for batch in progress_bar:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Track learning rate at each step\n",
        "        learning_rates.append(optimizer.param_groups[0]['lr'])\n",
        "        # Track loss at each step\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        progress_bar.set_postfix({\"Loss\": loss.item()})\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Loss: {epoch_loss / len(dataloader)}\")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"fine_tuned_legalbert_with_sbert\")\n",
        "tokenizer.save_pretrained(\"fine_tuned_legalbert_with_sbert\")\n",
        "print(\"Fine-tuning completed and model saved!\")\n",
        "\n",
        "# Plot learning rate graph\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(learning_rates)\n",
        "plt.title(\"Learning Rate over Training Steps\")\n",
        "plt.xlabel(\"Training Step\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "\n",
        "# Plot loss graph\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(losses)\n",
        "plt.title(\"Loss over Training Steps\")\n",
        "plt.xlabel(\"Training Step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "# Show the graphs\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nada3GbajLdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "from transformers import BertTokenizer, EncoderDecoderModel\n",
        "from tqdm import tqdm\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Paths\n",
        "preprocessed_folder = \"PreprocessedDataSet\"\n",
        "generated_folder = \"GeneratedSummaries\"\n",
        "reference_folder = \"ReferenceSummaries\"\n",
        "fine_tuned_model_path = \"fine_tuned_legalbert_with_sbert\"  # Path to the fine-tuned model and tokenizer\n",
        "\n",
        "# Initialize Tokenizer and Model from the fine-tuned model\n",
        "tokenizer = BertTokenizer.from_pretrained(fine_tuned_model_path)\n",
        "model = EncoderDecoderModel.from_pretrained(fine_tuned_model_path)\n",
        "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Ensure the GeneratedSummaries folder exists, and clean it if needed\n",
        "if os.path.exists(generated_folder):\n",
        "    shutil.rmtree(generated_folder)  # Remove existing folder if it exists\n",
        "os.makedirs(generated_folder)\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Function to chunk the text to fit the model's input size\n",
        "def chunk_text(text, chunk_size=512, overlap=50):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), chunk_size - overlap):\n",
        "        chunk = tokens[i:i + chunk_size]\n",
        "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
        "    return chunks\n",
        "\n",
        "# Function to generate summaries for each file and calculate ROUGE score\n",
        "def generate_summaries_and_evaluate():\n",
        "    all_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "    for file_name in tqdm(os.listdir(preprocessed_folder), desc=\"Generating Summaries and Evaluating\"):\n",
        "        if file_name.endswith(\".txt\"):\n",
        "            # Load the preprocessed text file\n",
        "            with open(os.path.join(preprocessed_folder, file_name), \"r\", encoding=\"utf-8\") as f:\n",
        "                input_text = f.read().strip()\n",
        "\n",
        "            # Chunk the input text if it exceeds the model's max input size\n",
        "            input_chunks = chunk_text(input_text)\n",
        "\n",
        "            # Generate summary for each chunk and concatenate them\n",
        "            generated_summary = \"\"\n",
        "            for chunk in input_chunks:\n",
        "                inputs = tokenizer(chunk, return_tensors=\"pt\", max_length=512, padding=True, truncation=True).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "                summary_ids = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], num_beams=4, max_length=150, early_stopping=True)\n",
        "\n",
        "                # Decode the summary and append it\n",
        "                summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "                generated_summary += summary + \" \"\n",
        "\n",
        "            # Save the generated summary into the GeneratedSummaries folder\n",
        "            with open(os.path.join(generated_folder, file_name), \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(generated_summary.strip())\n",
        "\n",
        "            # Load the reference summary\n",
        "            with open(os.path.join(reference_folder, file_name), \"r\", encoding=\"utf-8\") as f:\n",
        "                reference_summary = f.read().strip()\n",
        "\n",
        "            # Calculate ROUGE score between generated summary and reference summary\n",
        "            scores = scorer.score(reference_summary, generated_summary.strip())\n",
        "            all_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
        "            all_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
        "            all_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
        "\n",
        "            # Display the ROUGE scores for each file\n",
        "            print(f\"File: {file_name}\")\n",
        "            print(f\"ROUGE-1: {scores['rouge1'].fmeasure:.4f}\")\n",
        "            print(f\"ROUGE-2: {scores['rouge2'].fmeasure:.4f}\")\n",
        "            print(f\"ROUGE-L: {scores['rougeL'].fmeasure:.4f}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "    # Display the overall average scores\n",
        "    avg_rouge1 = sum(all_scores['rouge1']) / len(all_scores['rouge1'])\n",
        "    avg_rouge2 = sum(all_scores['rouge2']) / len(all_scores['rouge2'])\n",
        "    avg_rougeL = sum(all_scores['rougeL']) / len(all_scores['rougeL'])\n",
        "\n",
        "    print(\"Overall Average Scores:\")\n",
        "    print(f\"ROUGE-1: {avg_rouge1:.4f}\")\n",
        "    print(f\"ROUGE-2: {avg_rouge2:.4f}\")\n",
        "    print(f\"ROUGE-L: {avg_rougeL:.4f}\")\n",
        "\n",
        "# Generate the summaries and evaluate them\n",
        "generate_summaries_and_evaluate()\n",
        "print(\"Summaries generated and evaluated.\")\n"
      ],
      "metadata": {
        "id": "t8L39Pfzo3-J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}